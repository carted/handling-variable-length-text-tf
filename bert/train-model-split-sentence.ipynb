{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f943bf42",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9014279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44377fcf",
   "metadata": {},
   "source": [
    "## Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORDS_DIR = \"tfrecords-sentence-splitter\"\n",
    "BERT_MAX_SEQLEN = 512\n",
    "BERT_DIM = 768\n",
    "BATCH_SIZE = 64\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126bde2",
   "metadata": {},
   "source": [
    "## TFRecord parsing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d994d00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"summary\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_sentences\": tf.io.RaggedFeature(\n",
    "        value_key=\"summary_sentences_values\",\n",
    "        dtype=tf.int64,\n",
    "        partitions=[\n",
    "            tf.io.RaggedFeature.RowSplits(\"summary_sentences_splits_0\"),\n",
    "            tf.io.RaggedFeature.RowSplits(\"summary_sentences_splits_1\"),\n",
    "        ],\n",
    "    ),\n",
    "    \"summary_sentence_lens\": tf.io.RaggedFeature(\n",
    "        value_key=\"summary_sentence_lens_values\",\n",
    "        dtype=tf.int64,\n",
    "        partitions=[\n",
    "            tf.io.RaggedFeature.RowSplits(\"summary_sentence_lens_splits_0\"),\n",
    "        ],\n",
    "    ),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_example(example):\n",
    "    \"\"\"Parses a single TFRecord file.\"\"\"\n",
    "    features = tf.io.parse_single_example(example, feature_descriptions)\n",
    "\n",
    "    # Re-casting as int32 RaggedTensors\n",
    "    features[\"summary_sentences\"] = tf.cast(\n",
    "        features[\"summary_sentences\"].with_row_splits_dtype(tf.int64), tf.int32\n",
    "    )\n",
    "    features[\"summary_sentence_lens\"] = tf.cast(\n",
    "        features[\"summary_sentence_lens\"].with_row_splits_dtype(tf.int64), tf.int32\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInputUtils:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_preprocessor_path: str = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "        encoder_max_seqlen: int = BERT_MAX_SEQLEN,\n",
    "        dynamic_batching: bool = True,\n",
    "        data_max_seq_len: int = 0,\n",
    "    ):\n",
    "        \"\"\"Initializes a BERT model input preprocessing utility class.\"\"\"\n",
    "        self.bert_preprocessor_path = bert_preprocessor_path\n",
    "        self.preprocessor_module = hub.load(bert_preprocessor_path)\n",
    "        self.encoder_max_seqlen = encoder_max_seqlen\n",
    "\n",
    "        max_seq_len = tf.minimum(data_max_seq_len + 2, encoder_max_seqlen)\n",
    "        self.packer = hub.KerasLayer(\n",
    "            self.preprocessor_module.bert_pack_inputs,\n",
    "            arguments={\"seq_length\": max_seq_len},\n",
    "        )\n",
    "        self.dynamic_batching = tf.constant(dynamic_batching)\n",
    "\n",
    "    def pack_inputs(self, batch_tokens: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Prepares inputs for the BERT encoder\"\"\"\n",
    "        return self.packer([batch_tokens])\n",
    "\n",
    "    def init_packer_and_pack_inputs(\n",
    "        self, batch_tokens: tf.Tensor, batch_token_lens: tf.Tensor\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Prepares inputs for the BERT encoder.\"\"\"\n",
    "        max_token_len = tf.reduce_max(batch_token_lens)\n",
    "        packer = hub.KerasLayer(\n",
    "            self.preprocessor_module.bert_pack_inputs,\n",
    "            arguments={\n",
    "                \"seq_length\": tf.math.minimum(\n",
    "                    max_token_len + 2, self.encoder_max_seqlen\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "        return packer([batch_tokens])\n",
    "\n",
    "    def get_bert_inputs(self, batch, batch_size):\n",
    "        \"\"\"Generates padded BERT inputs for a given batch of tokenied\n",
    "        text features.\"\"\"\n",
    "\n",
    "        # Unravelling a batch of RaggedTensors\n",
    "        tokens = batch.pop(\"summary_sentences\").merge_dims(0, 1)\n",
    "\n",
    "        # obtain the BERT inputs\n",
    "        bert_inputs = tf.cond(\n",
    "            self.dynamic_batching,\n",
    "            lambda: self.init_packer_and_pack_inputs(\n",
    "                tokens, batch.pop(\"summary_sentence_lens\").flat_values\n",
    "            ),\n",
    "            lambda: self.pack_inputs(tokens),\n",
    "        )\n",
    "\n",
    "        return bert_inputs\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict[str, tf.Tensor]):\n",
    "        \"\"\"Applies batch level transformations to the data.\"\"\"\n",
    "        batch_size = tf.shape(batch[\"label\"])[0]\n",
    "\n",
    "        # generate padded BERT inputs for all the text features\n",
    "        batch[\"bert_inputs\"] = self.get_bert_inputs(batch, batch_size)\n",
    "\n",
    "        label = batch.pop(\"label\")\n",
    "        return batch, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20240ca5",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2287f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    split: str, batch_size: int, batch_preprocessor: Callable, shuffle: bool\n",
    "):\n",
    "    \"\"\"Prepares tf.data.Dataset objects from TFRecords.\"\"\"\n",
    "\n",
    "    ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/{split}-*.tfrecord\")\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset, cycle_length=AUTO, num_parallel_calls=AUTO\n",
    "    )\n",
    "\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    ds = ds.map(\n",
    "        read_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False\n",
    "    ).cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(batch_size * 10)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(batch_preprocessor, num_parallel_calls=AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab367d67",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_classifier(\n",
    "    proj_dim: int,\n",
    "    num_labels: int,\n",
    "):\n",
    "    \"\"\"Creates a simple classification model.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=(BERT_DIM,), dtype=tf.float32, name=\"cmlm_embeddings\")\n",
    "\n",
    "    projections = tf.keras.layers.Dense(proj_dim, activation=\"relu\")(inputs)\n",
    "    probs = tf.keras.layers.Dense(num_labels, activation=\"softmax\")(projections)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d7cc9",
   "metadata": {},
   "source": [
    "## Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreModelTrainer(tf.keras.Model):\n",
    "    \"\"\"Encapsulates the core model training logic.\"\"\"\n",
    "\n",
    "    def __init__(self, proj_dim, num_labels, encoder_path, train_encoder=False):\n",
    "        super(GenreModelTrainer, self).__init__()\n",
    "        self.predictor = genre_classifier(proj_dim, num_labels)\n",
    "        self.sentence_encoder = hub.KerasLayer(encoder_path)\n",
    "        self.sentence_encoder.trainable = train_encoder\n",
    "\n",
    "    def contiguous_group_average_vectors(self, vectors, groups):\n",
    "        \"\"\"Works iff sum(groups) == len(vectors)\n",
    "        Example:\n",
    "            Inputs: vectors: A dense 2D tensor of shape = (13, 3)\n",
    "                    groups : A dense 1D tensor with values [2, 5, 1, 4, 1]\n",
    "                    indicating that there are 5 groups.\n",
    "            Objective: Compute a 5x3 matrix where the first row\n",
    "                        is the average of the rows 0-1 of `vectors`,\n",
    "                        the second row is the average of rows 2-6 of\n",
    "                        vectors, the third row is the row 7 of vectors,\n",
    "                        the fourth row is the average of rows 8-11 of\n",
    "                        vectors and the fifth and final row is the row\n",
    "                        12 of vectors.\n",
    "            Logic: A selection mask matrix is generated\n",
    "                    mask = [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
    "                    This mask is then multiplied with `vectors` to get a\n",
    "                    matrix of shape (5, 3) called `summed_vectors` where\n",
    "                    each row contains the group sums.\n",
    "                    `summed_vectors` is then devided by `groups` to\n",
    "                    obtain the averages.\n",
    "        \"\"\"\n",
    "        groups = tf.expand_dims(tf.cast(groups, dtype=tf.int32), axis=1)\n",
    "        group_cumsum = tf.cumsum(groups)\n",
    "\n",
    "        mask = tf.repeat(\n",
    "            tf.expand_dims(tf.range(tf.shape(vectors)[0]), axis=0),\n",
    "            repeats=tf.shape(groups)[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        mask = tf.cast(mask < group_cumsum, dtype=tf.float32)\n",
    "\n",
    "        def complete_mask(mask):\n",
    "            neg_mask = tf.concat(\n",
    "                (tf.expand_dims(tf.ones_like(mask[0]), axis=0), 1 - mask[:-1]), axis=0\n",
    "            )\n",
    "            return mask * neg_mask\n",
    "\n",
    "        mask = tf.cond(\n",
    "            tf.greater(tf.shape(groups)[0], 1),\n",
    "            true_fn=lambda: complete_mask(mask),\n",
    "            false_fn=lambda: mask,\n",
    "        )\n",
    "\n",
    "        summed_vectors = tf.matmul(mask, vectors)\n",
    "        averaged_vectors = summed_vectors / tf.cast(groups, dtype=tf.float32)\n",
    "\n",
    "        return averaged_vectors\n",
    "\n",
    "    def compute_text_embeddings(self, features):\n",
    "        embeddings = self.sentence_encoder(features[\"bert_inputs\"])[\"pooled_output\"]\n",
    "        embeddings = self.contiguous_group_average_vectors(\n",
    "            embeddings, features[\"summary_num_sentences\"]\n",
    "        )\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Unpack the features and the labels.\n",
    "        features, labels = batch\n",
    "\n",
    "        # Compute embeddings for the text features.\n",
    "        embeddings = self.compute_text_embeddings(features)\n",
    "\n",
    "        # Main loop.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.predictor(embeddings, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions)\n",
    "\n",
    "        # Compute gradients and update the parameters.\n",
    "        learnable_params = self.predictor.trainable_variables\n",
    "        gradients = tape.gradient(loss, learnable_params)\n",
    "\n",
    "        # Apply the gradients to the parameters.\n",
    "        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        # Unpack the features and the labels.\n",
    "        features, labels = batch\n",
    "\n",
    "        # Get the predictions.\n",
    "        predictions = self.call(features)\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_loss(labels, predictions)\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, features):\n",
    "        embeddings = self.compute_text_embeddings(features)\n",
    "        return self.predictor(embeddings, training=False)\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        return self.call(data)\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        self.predictor.save_weights(\n",
    "            filepath, overwrite=overwrite, save_format=save_format, options=options\n",
    "        )\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
    "        self.predictor.load_weights(\n",
    "            filepath, by_name=by_name, skip_mismatch=skip_mismatch, options=options\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d9e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_ds: tf.data.Dataset,\n",
    "    valid_ds: tf.data.Dataset,\n",
    "    test_ds: tf.data.Dataset,\n",
    "    num_epochs: int,\n",
    "    run_name: str,\n",
    "    group_name: str,\n",
    "):\n",
    "    tfhub_model_uri = (\n",
    "        \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1\"\n",
    "    )\n",
    "    proj_dim = 128\n",
    "    num_labels = 27\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"batching-experiments\",\n",
    "        entity=\"carted\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "    )\n",
    "\n",
    "    trainer = GenreModelTrainer(proj_dim, num_labels, tfhub_model_uri, False)\n",
    "    trainer.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=\"accuracy\",\n",
    "    )\n",
    "    start = time.time()\n",
    "    trainer.fit(\n",
    "        train_ds,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[wandb.keras.WandbCallback()],\n",
    "    )\n",
    "    end = time.time()\n",
    "    wandb.log({\"model_training_time (seconds)\": end - start})\n",
    "\n",
    "    loss, acc = trainer.evaluate(test_ds)\n",
    "    wandb.log({\"test_loss\": loss})\n",
    "    wandb.log({\"test_acc\": acc})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9c7d",
   "metadata": {},
   "source": [
    "## Training with fixed batch length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da308ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the longest sequence in the training set\n",
    "ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/train-*.tfrecord\")\n",
    "ds = tf.data.TFRecordDataset(ds).map(read_example)\n",
    "max_seq_len = tf.cast(\n",
    "    tf.reduce_max(\n",
    "        [tf.reduce_max(datum[\"summary_sentence_lens\"].flat_values) for datum in ds]\n",
    "    ),\n",
    "    tf.int32,\n",
    ")\n",
    "print(f\"Longest token sequence in the training split: {max_seq_len.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eacf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_utility = ModelInputUtils(dynamic_batching=False, data_max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ec5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_ds = get_dataset(\n",
    "    split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_ds = get_dataset(\n",
    "    split=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = \"split-sentences-fixed-length-batching\"\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"cmlm-fixed-length-run:{i + 1}\"\n",
    "    train(train_ds, valid_ds, test_ds, NUM_EPOCHS, run_name, group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ff2e06",
   "metadata": {},
   "source": [
    "## Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117bbe5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "NUM_RUNS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15867b5f",
   "metadata": {},
   "source": [
    "## Training with variable batch length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_utility = ModelInputUtils(dynamic_batching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\n",
    "    split=\"train\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_ds = get_dataset(\n",
    "    split=\"val\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=False,\n",
    ")\n",
    "test_ds = get_dataset(\n",
    "    split=\"test\",\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_preprocessor=input_utility.preprocess_batch,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = \"split-sentences-variable-length-batching\"\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"cmlm-variable-length-run:{i + 1}\"\n",
    "    train(train_ds, valid_ds, test_ds, NUM_EPOCHS, run_name, group_name)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-var-len-text-py",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "interpreter": {
   "hash": "20d99275fbeb957608cf5adaee64ff23d07ebc4078dd173ca4cbf341a3a79b45"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f943bf42",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9014279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import wandb\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44377fcf",
   "metadata": {},
   "source": [
    "## Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d42dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORDS_DIR = \"gs://variable-length-sequences-tf/tfrecords-sentence-splitter\"\n",
    "BERT_MAX_SEQLEN = 512\n",
    "BERT_DIM = 768\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 5\n",
    "NUM_RUNS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126bde2",
   "metadata": {},
   "source": [
    "## TFRecord parsing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a40156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"summary\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_tokens\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_sentence_indices\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_num_sentences\": tf.io.FixedLenFeature([], dtype=tf.int64),\n",
    "    \"summary_tokens_len\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a5de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_composite(\n",
    "    serialized: bytes, type_spec: tf.RaggedTensorSpec\n",
    ") -> tf.Tensor:\n",
    "    \"\"\"Deserializes a serialised ragged tensor.\"\"\"\n",
    "\n",
    "    serialized = tf.io.parse_tensor(serialized, tf.string)\n",
    "    component_specs = tf.nest.flatten(type_spec, expand_composites=True)\n",
    "    components = [\n",
    "        tf.io.parse_tensor(serialized[i], spec.dtype)\n",
    "        for i, spec in enumerate(component_specs)\n",
    "    ]\n",
    "    return tf.nest.pack_sequence_as(type_spec, components, expand_composites=True)\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    \"\"\"Parses a single TFRecord file.\"\"\"\n",
    "    features = tf.io.parse_single_example(example, feature_descriptions)\n",
    "    features[\"summary_tokens\"] = deserialize_composite(\n",
    "        features.get(\"summary_tokens\"),\n",
    "        tf.RaggedTensorSpec(dtype=tf.int32, ragged_rank=2),\n",
    "    )\n",
    "    features[\"summary_sentence_indices\"] = deserialize_composite(\n",
    "        features.get(\"summary_sentence_indices\"),\n",
    "        tf.RaggedTensorSpec(dtype=tf.int32, ragged_rank=1),\n",
    "    )\n",
    "    features[\"summary_tokens_len\"] = deserialize_composite(\n",
    "        features.get(\"summary_tokens_len\"),\n",
    "        tf.RaggedTensorSpec(dtype=tf.int32, ragged_rank=1),\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce6c4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInputUtils:\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_preprocessor_path: str = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\",\n",
    "        encoder_max_seqlen: int = BERT_MAX_SEQLEN,\n",
    "        dynamic_batching: bool = True,\n",
    "        data_max_seq_len: int = None\n",
    "    ):\n",
    "        \"\"\"Initializes a BERT model input preprocessing utility class.\"\"\"\n",
    "        self.bert_preprocessor_path = bert_preprocessor_path\n",
    "        self.preprocessor_module = hub.load(bert_preprocessor_path)\n",
    "        self.encoder_max_seqlen = encoder_max_seqlen\n",
    "        self.dynamic_batching = dynamic_batching\n",
    "        if not self.dynamic_batching:\n",
    "            max_seq_len = tf.minimum(data_max_seq_len + 2, encoder_max_seqlen)\n",
    "            self.packer = hub.KerasLayer(\n",
    "                self.preprocessor_module.bert_pack_inputs,\n",
    "                arguments={\"max_seq_length\": max_seq_len}\n",
    "            )\n",
    "        \n",
    "    \n",
    "    def pack_inputs(self, batch_tokens: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Prepares inputs for the BERT encoder\"\"\"\n",
    "        return self.packer([batch_tokens])\n",
    "\n",
    "    def init_packer_and_pack_inputs(\n",
    "        self, batch_tokens: tf.Tensor, batch_token_lens: tf.Tensor\n",
    "    ) -> tf.Tensor:\n",
    "        \"\"\"Prepares inputs for the BERT encoder.\"\"\"\n",
    "        max_token_len = tf.reduce_max(batch_token_lens)\n",
    "        packer = hub.KerasLayer(\n",
    "            self.preprocessor_module.bert_pack_inputs,\n",
    "            arguments={\n",
    "                \"seq_length\": tf.math.minimum(\n",
    "                    max_token_len + 2, self.encoder_max_seqlen\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "        return packer([batch_tokens])\n",
    "\n",
    "    def unravel_ragged_batch(self, ragged_batch, ragged_idx, batch_lens, batch_size):\n",
    "        \"\"\"Flattens out a batch of ragged tensors by one level.\"\"\"\n",
    "        # create indices for each tensor in the batch\n",
    "        # for entries which have multiple ragged tensors, repeat their\n",
    "        # index once for each tensor in the entry\n",
    "        batch_idx = tf.repeat(tf.range(batch_size), batch_lens, axis=0)\n",
    "\n",
    "        # calculate length of the unravelled batch\n",
    "        unravelled_len = tf.reduce_sum(batch_lens)\n",
    "\n",
    "        # create a vector with alternating batch index and ragged tensor index\n",
    "        gather_nd_idx = tf.dynamic_stitch(\n",
    "            indices=[\n",
    "                tf.range(0, (unravelled_len * 2) - 1, 2, dtype=tf.int32),\n",
    "                tf.range(1, unravelled_len * 2, 2, dtype=tf.int32),\n",
    "            ],\n",
    "            data=[batch_idx, ragged_idx.flat_values],\n",
    "        )\n",
    "\n",
    "        # reshape the vector to obtain a unravelled_len x 2 matrix of indices\n",
    "        gather_nd_idx = tf.reshape(gather_nd_idx, shape=[-1, 2])\n",
    "\n",
    "        # obtain the flattened ragged batch using the index matrix\n",
    "        unravelled_tensors = tf.gather_nd(\n",
    "            ragged_batch, indices=gather_nd_idx, batch_dims=0\n",
    "        )\n",
    "\n",
    "        return unravelled_tensors\n",
    "\n",
    "    def get_bert_inputs(self, batch, batch_size):\n",
    "        \"\"\"Generates padded BERT inputs for a given batch of tokenied\n",
    "        text features.\"\"\"\n",
    "        # flatten out the RaggedTensor token batch.\n",
    "        tokens = self.unravel_ragged_batch(\n",
    "            batch.pop(\"summary_tokens\"),\n",
    "            batch.pop(\"summary_sentence_indices\"),\n",
    "            batch[\"summary_num_sentences\"],\n",
    "            batch_size,\n",
    "        )\n",
    "        # obtain the BERT inputs\n",
    "        batch[\"summary_tokens\"] = tokens\n",
    "        bert_inputs = tf.cond(\n",
    "            self.dynamic_batching, \n",
    "            self.init_packer_and_pack_inputs(\n",
    "                tokens, batch.pop(\"summary_tokens_len\").flat_values\n",
    "            ),\n",
    "            self.pack_inputs(tokens)\n",
    "        )\n",
    "        return bert_inputs\n",
    "\n",
    "    def preprocess_batch(self, batch: Dict[str, tf.Tensor]):\n",
    "        \"\"\"Applies batch level transformations to the data.\"\"\"\n",
    "        batch_size = tf.shape(batch[\"label\"])[0]\n",
    "\n",
    "        # generate padded BERT inputs for all the text features\n",
    "        batch[\"bert_inputs\"] = self.get_bert_inputs(batch, batch_size)\n",
    "\n",
    "        label = batch.pop(\"label\")\n",
    "        return batch, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20240ca5",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2287f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(split: str, batch_size: int, batch_preprocessor: Callable, shuffle: bool):\n",
    "    \"\"\"Prepares tf.data.Dataset objects from TFRecords.\"\"\"\n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "\n",
    "    ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/{split}-*.tfrecord\")\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset, cycle_length=3, num_parallel_calls=AUTO\n",
    "    )\n",
    "\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    ds = ds.map(\n",
    "        read_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False\n",
    "    ).cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(batch_size * 10)\n",
    "    ds = ds.batch(batch_size)\n",
    "    ds = ds.map(batch_preprocessor, num_parallel_calls=AUTO)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab367d67",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a1a330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_classifier(\n",
    "    proj_dim: int,\n",
    "    num_labels: int,\n",
    "):\n",
    "    \"\"\"Creates a simple classification model.\"\"\"\n",
    "    input = tf.keras.Input(shape=(BERT_DIM), dtype=tf.int32, name=\"cmlm_embeddings\")\n",
    "\n",
    "    projections = tf.keras.layers.Dense(proj_dim, activation=\"relu\")(input)\n",
    "    probs = tf.keras.layers.Dense(num_labels, activation=\"softmax\")(projections)\n",
    "    return tf.keras.Model(inputs=input, outputs=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34d7cc9",
   "metadata": {},
   "source": [
    "## Training routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b1660",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreModelTrainer(tf.keras.Model):\n",
    "    \"\"\"Encapsulates the core model training logic.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "        proj_dim, num_labels, encoder_path, train_encoder=False\n",
    "    ):\n",
    "        super(GenreModelTrainer, self).__init__()\n",
    "        self.predictor = genre_classifier(proj_dim, num_labels)\n",
    "        self.sentence_encoder = hub.KerasLayer(encoder_path)\n",
    "        self.sentence_encoder.trainable = train_encoder\n",
    "\n",
    "    def contiguous_group_average_vectors(self, vectors, groups):\n",
    "        \"\"\"Works iff sum(groups) == len(vectors)\n",
    "        Example:\n",
    "            Inputs: vectors: A dense 2D tensor of shape = (13, 3)\n",
    "                    groups : A dense 1D tensor with values [2, 5, 1, 4, 1]\n",
    "                    indicating that there are 5 groups.\n",
    "            Objective: Compute a 5x3 matrix where the first row\n",
    "                        is the average of the rows 0-1 of `vectors`,\n",
    "                        the second row is the average of rows 2-6 of\n",
    "                        vectors, the third row is the row 7 of vectors,\n",
    "                        the fourth row is the average of rows 8-11 of\n",
    "                        vectors and the fifth and final row is the row\n",
    "                        12 of vectors.\n",
    "            Logic: A selection mask matrix is generated\n",
    "                    mask = [[1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 0.]\n",
    "                            [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n",
    "                    This mask is then multiplied with `vectors` to get a\n",
    "                    matrix of shape (5, 3) called `summed_vectors` where\n",
    "                    each row contains the group sums.\n",
    "                    `summed_vectors` is then devided by `groups` to\n",
    "                    obtain the averages.\n",
    "        \"\"\"\n",
    "        groups = tf.expand_dims(tf.cast(groups, dtype=tf.int32), axis=1)\n",
    "        group_cumsum = tf.cumsum(groups)\n",
    "\n",
    "        mask = tf.repeat(\n",
    "            tf.expand_dims(tf.range(tf.shape(vectors)[0]), axis=0),\n",
    "            repeats=tf.shape(groups)[0],\n",
    "            axis=0,\n",
    "        )\n",
    "        mask = tf.cast(mask < group_cumsum, dtype=tf.float32)\n",
    "\n",
    "        def complete_mask(mask):\n",
    "            neg_mask = tf.concat(\n",
    "                (tf.expand_dims(tf.ones_like(mask[0]), axis=0), 1 - mask[:-1]), axis=0\n",
    "            )\n",
    "            return mask * neg_mask\n",
    "\n",
    "        mask = tf.cond(\n",
    "            tf.greater(tf.shape(groups)[0], 1),\n",
    "            true_fn=lambda: complete_mask(mask),\n",
    "            false_fn=lambda: mask,\n",
    "        )\n",
    "\n",
    "        summed_vectors = tf.matmul(mask, vectors)\n",
    "        averaged_vectors = summed_vectors / tf.cast(groups, dtype=tf.float32)\n",
    "\n",
    "        return averaged_vectors\n",
    "\n",
    "    def compute_text_embeddings(self, features):\n",
    "        embeddings = self.bert_encoder(features[f\"bert_inputs\"])\n",
    "        embeddings = self.contiguous_group_average_vectors(\n",
    "            embeddings, features[\"summary_num_sentences\"]\n",
    "        )\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Unpack the features and the labels.\n",
    "        features, labels = batch\n",
    "\n",
    "        # Compute embeddings for the text features.\n",
    "        embeddings = self.compute_text_embeddings(features)\n",
    "\n",
    "        # Main loop.\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.predictor(embeddings, training=True)\n",
    "            loss = self.compiled_loss(labels, predictions)\n",
    "\n",
    "        # Compute gradients and update the parameters.\n",
    "        learnable_params = self.predictor.trainable_variables\n",
    "        gradients = tape.gradient(loss, learnable_params)\n",
    "\n",
    "        # Non-SAM update.\n",
    "        self.optimizer.apply_gradients(zip(gradients, learnable_params))\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        # Unpack the features and the labels.\n",
    "        features, labels = batch\n",
    "\n",
    "        # Get the predictions.\n",
    "        predictions = self.call(features)\n",
    "\n",
    "        # Report progress.\n",
    "        self.compiled_loss(labels, predictions)\n",
    "        self.compiled_metrics.update_state(labels, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, features):\n",
    "        embeddings = self.compute_text_embeddings(features)\n",
    "        return self.predictor(embeddings, training=False)\n",
    "\n",
    "    def predict_step(self, data):\n",
    "        return self.call(data)\n",
    "\n",
    "    def save_weights(self, filepath, overwrite=True, save_format=None, options=None):\n",
    "        self.predictor.save_weights(\n",
    "            filepath, overwrite=overwrite, save_format=save_format, options=options\n",
    "        )\n",
    "\n",
    "    def load_weights(self, filepath, by_name=False, skip_mismatch=False, options=None):\n",
    "        self.predictor.load_weights(\n",
    "            filepath, by_name=by_name, skip_mismatch=skip_mismatch, options=options\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0d9e34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    train_ds: tf.data.Dataset,\n",
    "    valid_ds: tf.data.Dataset,\n",
    "    test_ds: tf.data.Dataset,\n",
    "    num_epochs: int,\n",
    "    run_name: str,\n",
    "    group_name: str,\n",
    "):\n",
    "    tfhub_model_uri = \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1\"\n",
    "    # bert_inputs = [\"input_word_ids\", \"input_type_ids\", \"input_mask\"]\n",
    "    proj_dim = 128\n",
    "    num_labels = 27\n",
    "    \n",
    "\n",
    "\n",
    "    wandb.init(\n",
    "        project=\"batching-experiments\",\n",
    "        entity=\"carted\",\n",
    "        name=run_name,\n",
    "        group=group_name,\n",
    "    )\n",
    "\n",
    "    trainer = GenreModelTrainer(proj_dim, num_labels, tfhub_model_uri, False)\n",
    "    trainer.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\"\n",
    "    )\n",
    "    start = time.time()\n",
    "    train.fit(\n",
    "        train_ds,\n",
    "        epochs=num_epochs,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[wandb.keras.WandbCallback()],\n",
    "    )\n",
    "    end = time.time()\n",
    "    wandb.log({\"model_training_time (seconds)\": end - start})\n",
    "\n",
    "    loss, acc = trainer.evaluate(test_ds)\n",
    "    wandb.log({\"test_loss\": loss})\n",
    "    wandb.log({\"test_acc\": acc})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ab9c7d",
   "metadata": {},
   "source": [
    "## Training with fixed batch length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da308ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest token sequence in the training split: 549\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sequence in the training set\n",
    "ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/train-*.tfrecord\")\n",
    "ds = tf.data.TFRecordDataset(ds).map(read_example)\n",
    "max_seq_len = tf.cast(\n",
    "    tf.reduce_max([tf.reduce_max(datum[\"summary_tokens_len\"].flat_values) for datum in ds]), tf.int32\n",
    ")\n",
    "print(f\"Longest token sequence in the training split: {max_seq_len.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41eacf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_utility = ModelInputUtils(dynamic_batching=False, data_max_seq_len=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ec5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(split=\"train\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=True)\n",
    "valid_ds = get_dataset(split=\"val\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=False)\n",
    "test_ds = get_dataset(split=\"test\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedb0ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = \"split-sentences-fixed-length-batching\"\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"cmlm-fixed-length-run:{i + 1}\"\n",
    "    train(train_ds, valid_ds, test_ds, NUM_EPOCHS, run_name, group_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15867b5f",
   "metadata": {},
   "source": [
    "## Training with variable batch length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da8ff8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_utility = ModelInputUtils(dynamic_batching=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2588588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(split=\"train\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=True)\n",
    "valid_ds = get_dataset(split=\"val\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=False)\n",
    "test_ds = get_dataset(split=\"test\", batch_size=BATCH_SIZE, batch_preprocessor=input_utility.preprocess_batch, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = \"split-sentences-variable-length-batching\"\n",
    "\n",
    "for i in range(NUM_RUNS):\n",
    "    run_name = f\"cmlm-variable-length-run:{i + 1}\"\n",
    "    train(train_ds, valid_ds, test_ds, NUM_EPOCHS, run_name, group_name)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-6.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m81"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

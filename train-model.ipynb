{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f943bf42",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9014279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as tft\n",
    "import tensorflow as tf\n",
    "import wandb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "SEED = 42\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44377fcf",
   "metadata": {},
   "source": [
    "## Contants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d42dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFRECORDS_DIR = \"gs://variable-length-sequences-tf/tfrecords\"\n",
    "BERT_MAX_SEQLEN = 512\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126bde2",
   "metadata": {},
   "source": [
    "## TFRecord parsing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a40156c",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "    \"summary\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_tokens\": tf.io.FixedLenFeature([], dtype=tf.string),\n",
    "    \"summary_tokens_len\": tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "    \"label\": tf.io.FixedLenFeature([1], dtype=tf.int64),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47a5de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_composite(serialized, type_spec):\n",
    "    \"\"\"Parses a serialized Ragged features and retains the original structure.\"\"\"\n",
    "    serialized = tf.io.parse_tensor(serialized, tf.string)\n",
    "    component_specs = tf.nest.flatten(type_spec, expand_composites=True)\n",
    "    components = [\n",
    "        tf.io.parse_tensor(serialized[i], spec.dtype)\n",
    "        for i, spec in enumerate(component_specs)\n",
    "    ]\n",
    "    return tf.nest.pack_sequence_as(type_spec, components, expand_composites=True)\n",
    "\n",
    "\n",
    "def read_example(example):\n",
    "    \"\"\"Parses a single TFRecord file.\"\"\"\n",
    "    features = tf.io.parse_single_example(example, feature_descriptions)\n",
    "    features[\"summary_tokens\"] = deserialize_composite(\n",
    "        features.get(\"summary_tokens\"),\n",
    "        tf.RaggedTensorSpec(dtype=tf.int32, ragged_rank=2),\n",
    "    )\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce6c4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_text_preprocessor(preprocessor_path: str) -> Callable:\n",
    "    \"\"\" Decorator to set the desired preprocessor for a\n",
    "        function from a TensorFlow Hub URL.\n",
    "        \n",
    "    Arguments:\n",
    "        preprocessor_path {str} -- URL of the TF-Hub preprocessor.\n",
    "    \n",
    "    Returns:\n",
    "        Callable -- A function with the `preprocessor` attribute set.\n",
    "    \"\"\"\n",
    "    def decoration(func: Callable):\n",
    "        # Loading the preprocessor from TF-Hub\n",
    "        preprocessor = hub.load(preprocessor_path)\n",
    "        \n",
    "        # Setting an attribute called `preprocessor` to\n",
    "        # the passed function\n",
    "        func.preprocessor = preprocessor\n",
    "        return func\n",
    "    return decoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4379e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-17 04:59:09.355675: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-17 04:59:10.016078: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38414 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:00:04.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "@set_text_preprocessor(\n",
    "    preprocessor_path=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    ")\n",
    "def preprocess_variable_batch(batch):\n",
    "    \"\"\"Batch processing utility.\"\"\"\n",
    "    text_tokens_max_len = tf.cast(\n",
    "        tf.math.reduce_max(batch[\"summary_tokens_len\"]), dtype=tf.int32,\n",
    "    )\n",
    "\n",
    "    # Generating the inputs for the BERT model.\n",
    "    bert_input_packer = hub.KerasLayer(\n",
    "        preprocess_variable_batch.preprocessor.bert_pack_inputs,\n",
    "        arguments={\"seq_length\": tf.minimum(text_tokens_max_len, BERT_MAX_SEQLEN)},\n",
    "    )\n",
    "    bert_packed_text = bert_input_packer(\n",
    "        [tf.squeeze(batch.pop(\"summary_tokens\"), axis=1)]\n",
    "    )\n",
    "\n",
    "    labels = batch.pop(\"label\")\n",
    "    return bert_packed_text, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16af736b-2e31-43fd-be42-b62187591618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest token sequence in the training split: 2947\n"
     ]
    }
   ],
   "source": [
    "# Find the longest sequence in the training set\n",
    "ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/train-*.tfrecord\")\n",
    "ds = tf.data.TFRecordDataset(ds).map(\n",
    "    lambda example: tf.io.parse_single_example(\n",
    "        example, feature_descriptions\n",
    "    )\n",
    ")\n",
    "max_seq_len = max([datum[\"summary_tokens_len\"] for datum in ds])\n",
    "print(f\"Longest token sequence in the training split: {max_seq_len.numpy()[0]}\")\n",
    "\n",
    "\n",
    "# Use the bert packer for packing input batches\n",
    "@set_text_preprocessor(\n",
    "    preprocessor_path=\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
    ")\n",
    "def preprocess_fixed_batch(batch):\n",
    "    \"\"\"Batch processing utility.\"\"\"\n",
    "    \n",
    "    # Create a bert input packer using it\n",
    "    if not hasattr(\"fixed_len_bert_packer\"):\n",
    "        preprocess_fixed_batch.fixed_len_bert_packer = hub.KerasLayer(\n",
    "            preprocess_fixed_batch.preprocessor.bert_pack_inputs,\n",
    "            arguments={\"seq_length\": min(max_seq_len, BERT_MAX_SEQLEN)}\n",
    "        )\n",
    "    # Generating the inputs for the BERT model.\n",
    "    bert_packed_text = preprocess_fixed_batch.fixed_len_bert_packer(\n",
    "        [tf.squeeze(batch.pop(\"summary_tokens\"), axis=1)]\n",
    "    )\n",
    "\n",
    "    labels = batch.pop(\"label\")\n",
    "    return bert_packed_text, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20240ca5",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2287f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(\n",
    "    split: str,\n",
    "    batch_size: int,\n",
    "    batch_preprocessor: Callable,\n",
    "    shuffle: bool\n",
    "):\n",
    "    \"\"\"Prepares tf.data.Dataset objects from TFRecords.\"\"\"\n",
    "    ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/{split}-*.tfrecord\")\n",
    "    ds = ds.interleave(\n",
    "        tf.data.TFRecordDataset,\n",
    "        cycle_length=tf.data.AUTOTUNE,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    ).map(\n",
    "        read_example,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "        deterministic=False\n",
    "    ).cache()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(batch_size * 10)\n",
    "    ds = ds.batch(\n",
    "        batch_size\n",
    "    ).map(\n",
    "        batch_preprocessor,\n",
    "        num_parallel_calls=tf.data.AUTOTUNE\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59501ed9",
   "metadata": {},
   "source": [
    "## Analyzing the maximum sequence lengths over training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d95a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis_ds = tf.data.Dataset.list_files(f\"{TFRECORDS_DIR}/train-*.tfrecord\")\n",
    "# analysis_ds = analysis_ds.interleave(\n",
    "#     tf.data.TFRecordDataset, cycle_length=3, num_parallel_calls=tf.data.AUTOTUNE\n",
    "# )\n",
    "# analysis_ds = analysis_ds.map(\n",
    "#     read_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False\n",
    "# )\n",
    "# analysis_ds = analysis_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d472f4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_seqlens = []\n",
    "# batches = 0\n",
    "\n",
    "\n",
    "# for batch in analysis_ds:\n",
    "#     max_seqlens.append(\n",
    "#         int(tf.cast(tf.math.reduce_max(batch[\"summary_tokens_len\"]), dtype=tf.int32,))\n",
    "#     )\n",
    "#     batches += 1\n",
    "\n",
    "# plt.plot(np.arange(batches), max_seqlens)\n",
    "# plt.xlabel(\"Batch #\", fontsize=14)\n",
    "# plt.ylabel(\"Maximum sequence lengths\", fontsize=14)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2aca18",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "816f420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genre_classifier(\n",
    "    encoder_path: str,\n",
    "    input_features: List[str],\n",
    "    train_encoder: bool,\n",
    "    proj_dim: int,\n",
    "    num_labels: int\n",
    "):\n",
    "    \"\"\"Creates a simple classification model.\"\"\"\n",
    "    sentence_encoder = hub.KerasLayer(encoder_path)\n",
    "    sentence_encoder.trainable = train_encoder\n",
    "\n",
    "    inputs = {\n",
    "        feature_name: tf.keras.Input(\n",
    "            shape=(None,), dtype=tf.int32, name=feature_name\n",
    "        )\n",
    "        for feature_name in input_features\n",
    "    }\n",
    "\n",
    "    text_encodings = sentence_encoder(inputs)\n",
    "    projections = tf.keras.layers.Dense(proj_dim, activation=\"relu\")(text_encodings[\"pooled_output\"])\n",
    "    probs = tf.keras.layers.Dense(num_labels, activation=\"softmax\")(projections)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b5831bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmlm_uri = \"https://tfhub.dev/google/universal-sentence-encoder-cmlm/en-base/1\"\n",
    "cmlm_inputs = [\"input_word_ids\", \"input_type_ids\", \"input_mask\"]\n",
    "proj_dim = 128\n",
    "num_labels = 27\n",
    "\n",
    "num_epochs = 5\n",
    "num_runs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bcc099-fd26-4167-b4f5-2200bda269e3",
   "metadata": {},
   "source": [
    "## Training with variable batch width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2a87002-e162-4e05-ad96-4b5af895f68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\"train\", BATCH_SIZE, preprocess_variable_batch, True)\n",
    "valid_ds = get_dataset(\"val\", BATCH_SIZE, preprocess_variable_batch, False)\n",
    "test_ds = get_dataset(\"test\", BATCH_SIZE, preprocess_variable_batch, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89894dea-1e54-4fb1-b377-1387c1196ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3q7wh2pt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 23073... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "ERROR:root:dropped chunk 404 Client Error: Not Found for url: https://api.wandb.ai/files/carted/batching-experiments/3q7wh2pt/file_stream\n",
      "NoneType: None\n",
      "wandb: ERROR Error while calling W&B API: Error 1062: Duplicate entry '604596-3q7wh2pt' for key 'PRIMARY' (<Response [409]>)\n",
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/apis/normalize.py\", line 24, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 1298, in upsert_run\n",
      "    response = self.gql(mutation, variable_values=variable_values, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/lib/retry.py\", line 102, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 147, in execute\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/six.py\", line 719, in reraise\n",
      "    raise value\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal_api.py\", line 141, in execute\n",
      "    return self.client.execute(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/vendor/gql-0.2.0/gql/transport/requests.py\", line 39, in execute\n",
      "    request.raise_for_status()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/requests/models.py\", line 953, in raise_for_status\n",
      "    raise HTTPError(http_error_msg, response=self)\n",
      "requests.exceptions.HTTPError: 409 Client Error: Conflict for url: https://api.wandb.ai/graphql\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 52, in run\n",
      "    self._run()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 291, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 236, in send\n",
      "    send_handler(record)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 250, in send_request\n",
      "    send_handler(record)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 403, in send_request_defer\n",
      "    self.debounce()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 315, in debounce\n",
      "    self._debounce_config()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 321, in _debounce_config\n",
      "    self._api.upsert_run(\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/apis/normalize.py\", line 26, in wrapper\n",
      "    raise CommError(err.response, err)\n",
      "wandb.errors.CommError: <Response [409]>\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: /tmp/ipykernel_22791/2880621911.py 2 <module>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1788, in _atexit_cleanup\n",
      "    self._on_finish()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1971, in _on_finish\n",
      "    self._poll_exit_response = self._wait_for_finish()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1911, in _wait_for_finish\n",
      "    poll_exit_resp = self._backend.interface.communicate_poll_exit()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 586, in communicate_poll_exit\n",
      "    resp = self._communicate_poll_exit(poll_exit)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 402, in _communicate_poll_exit\n",
      "    result = self._communicate(rec)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 213, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 218, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 931, in init\n",
      "    run = wi.init()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\", line 437, in init\n",
      "    self._wl._global_run_stack[-1].finish()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1463, in finish\n",
      "    self._atexit_cleanup(exit_code=exit_code)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1797, in _atexit_cleanup\n",
      "    self._backend.cleanup()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/backend/backend.py\", line 245, in cleanup\n",
      "    self.interface.join()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 458, in join\n",
      "    super().join()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 599, in join\n",
      "    _ = self._communicate_shutdown()\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 455, in _communicate_shutdown\n",
      "    _ = self._communicate(record)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 213, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\", line 218, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Abnormal program exit\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "problem",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   1787\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1788\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1789\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mki\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1970\u001b[0m         \u001b[0;31m# Wait for data to be synced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1971\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_exit_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_wait_for_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1910\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m                 \u001b[0mpoll_exit_resp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate_poll_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"got exit ret: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll_exit_resp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mcommunicate_poll_exit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0mpoll_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPollExitRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_poll_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_exit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_poll_exit\u001b[0;34m(self, poll_exit)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_exit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_exit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    212\u001b[0m     ) -> Optional[pb.Result]:\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_async\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_router\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_and_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_atexit_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_global_run_stack\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   1796\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_console_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Problem finishing run\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/backend/backend.py\u001b[0m in \u001b[0;36mcleanup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwandb_process\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_shutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_shutdown\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mrecord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate\u001b[0;34m(self, rec, timeout, local)\u001b[0m\n\u001b[1;32m    212\u001b[0m     ) -> Optional[pb.Result]:\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_communicate_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/interface/interface_shared.py\u001b[0m in \u001b[0;36m_communicate_async\u001b[0;34m(self, rec, local)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_check\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_router\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_and_receive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22791/2880621911.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_runs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     wandb.init(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"batching-experiments\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"carted\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"variable-legth-run:{i + 1}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexcept_exit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m             \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"problem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_seen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/var-len-text/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: problem"
     ]
    }
   ],
   "source": [
    "for i in range(num_runs):\n",
    "    wandb.init(\n",
    "        project=\"batching-experiments\",\n",
    "        entity=\"carted\",\n",
    "        name=f\"variable-legth-run:{i + 1}\",\n",
    "        group=\"variable-length-batching\",\n",
    "    )\n",
    "    model = genre_classifier(cmlm_uri, cmlm_inputs, False, proj_dim, num_labels)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=5,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[wandb.keras.WandbCallback()]\n",
    "    )\n",
    "    end = time.time()\n",
    "    wandb.log({\"model_training_time (seconds)\": end - start})\n",
    "    \n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    wandb.log({\"test_loss\": loss})\n",
    "    wandb.log({\"test_acc\": acc})\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb5d2b-4ff7-41ae-bdba-6559623d4b47",
   "metadata": {},
   "source": [
    "## Training with fixed length batch width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d9ffd9-6446-4987-ac52-2c33a157238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = get_dataset(\"train\", BATCH_SIZE, preprocess_fixed_batch, True)\n",
    "valid_ds = get_dataset(\"val\", BATCH_SIZE, preprocess_fixed_batch, False)\n",
    "test_ds = get_dataset(\"test\", BATCH_SIZE, preprocess_fixed_batch, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d624092-1341-43f6-ab16-48642473bc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_runs):\n",
    "    wandb.init(\n",
    "        project=\"batching-experiments\",\n",
    "        entity=\"carted\",\n",
    "        name=f\"fixed-legth-run:{i + 1},\n",
    "        group=\"fixed-length-batching,\n",
    "        config=training_config.to_dict(),\n",
    "    )\n",
    "    model = genre_classifier(cmlm_uri, cmlm_inputs, False, proj_dim, num_labels)\n",
    "    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\")\n",
    "    start = time.time()\n",
    "    model.fit(\n",
    "        train_ds,\n",
    "        epochs=5,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[wandb.keras.WandbCallback()]\n",
    "    )\n",
    "    end = time.time()\n",
    "    wandb.log({\"model_training_time (seconds)\": end - start})\n",
    "    \n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    wandb.log({\"test_loss\": loss})\n",
    "    wandb.log({\"test_acc\": acc})\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348ba0b-3620-44e3-a709-c947a27d646c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-var-len-text-py",
   "name": "tf2-gpu.2-6.m84",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m84"
  },
  "kernelspec": {
   "display_name": "Python [conda env:var-len-text]",
   "language": "python",
   "name": "conda-env-var-len-text-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
